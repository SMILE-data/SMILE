torchrun --nproc_per_node=4 --master_port=20004 fastchat/train/train_mem.py --model_name_or_path /local_data2/hyun/model_weights/13B_converts --data_path /home/hyun/project/LLM/FastChat/playground/cross_validation/sitcom/sitcom_reasoning_train_5.json --bf16 True --output_dir /local_data2/sung/checkpoints/sitcom_5 --num_train_epochs 36 --per_device_train_batch_size 3 --per_device_eval_batch_size 2 --gradient_accumulation_steps 16 --evaluation_strategy "no" --save_strategy "steps" --save_steps 10 --save_total_limit 10 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type "cosine" --logging_steps 1 --fsdp "full_shard auto_wrap" --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' --tf32 True --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True
#/local_data2/hyun/model_weights/13B_converts
#/local_data2/hyun/model_weights/vicuna_13B